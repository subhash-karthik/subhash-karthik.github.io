---
title: "Quality Analysis of Wine "
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE,echo = TRUE,message = FALSE,warning = FALSE)
```


## Synopsis
The Goal of this project is to apply statistical learning methods to answer the following interesting questions:

* Which constituents(pedictors) found in wine are associated with the wine quality(response)
* Understanding the replationship between the response and predictors

Relavent information:
The data is obtained from the source [UCI ML datbase](https://archive.ics.uci.edu/ml/datasets/Wine).The data is compiled from results of chemical analyses of wines grown in same region in italy using differnet cultivars.
The dataset has 1599 observation with 12 variables and "quality" is the interested respone.

##Overview of Analysis approach
1. Data Preprocessing
2. Forward step-wise selection 
3. Model-selection using K-fold crossvalidation
4. Inferences from the best-final model

###Data preprocessing
The data is loaded as wine, and the response variable is plotted.Then we examine the preditor variables and observations containing "NA" values are omitted.The data is then split into Train and Test dataset.The Train dataset is used for learning the parameters related to the model and the test data is used for final evaluation.   
```{r}
library(readr)
library(knitr)
wine <- read_delim("F:/subhash/UCI datasets/regerssion/Wine quality/winequality-red.csv", 
    ";", escape_double = FALSE, trim_ws = TRUE)
```

```{r}
kable(head(wine),caption = "Wine dataframe")
hist(wine$quality,breaks =5,xlab="wine quality",ylab = "frequency",main = "Histogram of wine quality")
fivenum(wine$quality)
```
It is noted that the wine quality has a normal distribution(approx.) and the minimum and maximum values are 3 and 8 respectively, wit median at 6.Next we look at summary of our data to find are there any NA/missing values.

```{r}
summary(wine)
```
We see there are 2 NA values in the "total sulfur dioxide" variable, this missing values are handled by removing the observations corresponding to it.We'll also look the description of the variables(understood from the var names)and their type

Attribute           | variable type
------------------- |------------
Fixed Acidity       |Continuous
Volatile Acidity    |Continuous
Citric acid         |Continuous
Residual sugar      |Continuous
Chlorides           |Continuous
free sulfur dioxide |Continuous
total sulfur dioxide|Continuous
density             |Continuous
pH                  |Continuous
sulphates           |Continuous
alcohol             |Continuous  
quality             |Continuous
```{r}
wine<-na.omit(wine)
set.seed(1)
ind=sample(seq(1597),1280,replace=FALSE)
wine_train<-wine[ind,]
wine_test<-wine[-ind,]
```

###Linear model selection
Here we use forward stepwise selection, a computational efficient method to find the best set of predictors related to the respnse varible.
Brief overview: It begins with a model containing no predictors, and then adds predictors to the model one at a time, until all predictors are in the model.At each step we have p-k models where p refers to total number of predictors and k represents the step index.We have approximately $p^{2}$ models which is futher reduced to $p$ models.
```{r}
library(leaps)
regfit.fwd=regsubsets(quality~.,data=wine_train,nvmax=11,method="forward")
summary(regfit.fwd)
```
Here we can the best model of containing p=1,2,...,11 predictors variable and the actual varibles comprising the model.We plot a $C_{p}$ value for all the models found,and look for the lowest $C_{p}$ value as it corresponds to a lower test MSE.
```{r}
plot(regfit.fwd,scale="Cp",col = "blue")
```
From the above plot we see a lowest $C_{p}$ value around 6.7 comprising of predictor variable-
* Volatile acidity,chrolides,free sulphur dioxide,total sulphur dixide, pH,sulphates,alcohol.
* total number of predictor variables-7

###Model selection using 10 fold cross-validation
Training set MSE is generally an underestimate of the test MSE.This because when we fit a model to the training data using least squares we find the parameters for which the model minimizes the train MSE.So training error decreases as we increase more number of variables.So by picking a model with lowest train MSE, we may overfit the model.  
There are number of techniques for adjusting the training error for the model size.Few pouplar approaches are
* Akaike Information Criterion (AIC)
* Bayesian Information Criterion (BIC)
* adjusted R-squared
* Mallow's $C_{p}$
* k-fold Cross validation
We have seen earlier to use the $C_{p}$ criteria to find a best model.Now we'll use 10-fold cross validation to pick the best model.The major advantage of using cross-validation is it doesn't require $\sigma^{2}$, variance of the irreducible error.
```{r}
predict.regsubsets=function(object,newdata,id,...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  mat[,names(coefi)]%*%coefi
}
```

```{r}
set.seed(2)
folds=sample(rep(1:10,length=nrow(wine_train)))
table(folds)
cv.errors=matrix(NA,10,11)
for(k in 1:10){
  best.fit=regsubsets(quality~.,data=wine_train[folds!=k,],nvmax=11,method="forward")
  for(i in 1:11){
    pred=predict(best.fit,wine_train[folds==k,],id=i)
    cv.errors[k,i]=mean( (wine_train$quality[folds==k]-pred)^2)
  }
}
rmse.cv=sqrt(apply(cv.errors,2,mean))
rmse.train=sqrt(best.fit$rss/(128*9))
plot(rmse.cv,pch=19,type="b",col="red",ylab = "Root MSE",ylim=c(0.64,0.71))
points(rmse.train[-1],col="blue",pch=19,type="b")
legend("topright",legend=c("Training","Validation"),col=c("blue","red"),pch=19)
```

As we expect, the training error goes down monotonically as the model gets bigger, but not so for the validation error.The cross-validated method results in a 7 variable model,similar to the mallow $C_{p}$ approach.Lets check out the coeeficients of the best model.
```{r}
best.train=regsubsets(quality~.,data=wine_train,nvmax=11,method="forward")
id=which(rmse.cv==min(rmse.cv))
coef(best.train,id=id)
```

Next we test this model of the unused wine_test dataset comprising of 317 observations.

```{r}
pred.test=predict.regsubsets(best.train,wine_test[],id=id)
test.error<-mean( (wine_test$quality-pred.test)^2)
test.error
plot((wine_test$quality-round(pred.test)),xlab="observations",ylab="deviation from actual quality",col="blue")
table(abs(wine_test$quality-round(pred.test)))
```
From the counts table we see our model has performed well as it accurately predict quality of 307/317 obseravtions within 1 absolute quality difference, test MSE=0.4535

### Inferences
```{r}
plot(wine_train$alcohol,wine_train$quality)
points(wine_train$alcohol[wine_train$quality>6],wine_train$quality[wine_train$quality>6],col="blue")
```



